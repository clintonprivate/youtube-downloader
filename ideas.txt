Melisma is what it's called when a note changes in the same syllable. Melody similarity matching. Create a folder named challenges and do syllables separation on phrase.

A challenge is compare 2 arrays and measure similarity.

One thing we can ask is, just get it to give this output. But that makes the task a little obvious. We can take a segment we want to display an output. Ask a person to create machine learning that splits a song into phrases. Just ask for correct duration.

We can just improve it until we find it difficult to think of any more adjustments and be skipping to the next phrase from now on. Results so far.
['-', 'F4', 'D#4', 'D#4', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'G4', 'A#4', 'F4', 'F4', 'F4', 'C#4', 'C4', 'C#4', 'B4', 'C4', 'B4', 'A#4', 'A#4', 'D#4', 'D#4', 'E4', 'F4', 'B4']
['-', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'F4', 'G4', 'G4', 'F4', 'F4', 'F4', 'F4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4']
2. ['-', 'D#4', 'D#4', 'D#4', 'D#4', 'F4', 'F4', 'F4', 'F4', 'G4', 'G4', 'G4', 'G4', 'F4', 'F4', 'C4', 'C4',
'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4', 'C4']

Revert it back to github version and skip to the next phrase. We could also check for consonants and use that to separate repeated notes. That idea won't work so we'll ignore repetitions for now and say that the first phrase has been completed. An idea would be to loop through segments listening checking for hints at a break in sound, which indicates the same note repeated twice. Write the completely correct answer down in correct outputs text file. Completely correct answer: ['-', '-', '-', '-', '-', '-', 'D4', 'D4', 'E4', 'E4', 'E4', 'E4', 'B4', 'B4', 'B4', 'B4', '*', 'B4', 'B4', 'D4', 'D4', '*', 'D4', 'D4', '*', 'D4', 'D4', 'D4', 'D4', 'E4', 'E4', 'E4', 'E4']. Go to noteflight and write what the correct result should be completely and add repetitions to our known melody. After that, how do we detect repeated note durations? Next we remove the first note and add the repetitions to the known melody. It worked and this is the result so far. ['-', '-', '-', '-', '-', 'D4', 'D4', 'D4', 'E4', 'E4', 'E4', 'E4', 'B4', 'B4', 'B4', 'B4', 'B4', 'B4', 'D4', 'D4', 'D4', 'D4', 'D4', 
'D4', 'D4', 'D4', 'E4', 'E4', 'E4', 'E4']. Loop through segment detection. Apart from potential rest in the beginning, compare if note is closer to current note or next note, then adjust to whichever it's closer to and move to the next note. Remove repetition from the known melody. ['-', '-', '-', '-', '-', 'C#4', 'C#4', 'C#4', 'E4', 'E4', 'E4', 'E4', 'B4', 'B4', 'B4', 'A4', 'B4', 'B4', 'D4', 'C4', 'C#4', 'D4', 'D4', 'D4', 'C4', 'C4', 'E4', 'E4', 'E4', 'D4'] knownMelody = ["D4", "E4", "B4", "D4", "E4"]. Give the correct melody. Determine the fundamental frequency. Use the array and adjust based on the known melody. Don't want to use machine learning. Remove rounding logic and write down the result without it here. Possibly if a note is not contained in the key (any accidental but f sharp).

['-', '-', '-', '-', '-', 'D4', 'D4', 'D4', 'E4', 'E4', 'E4', 'E4', 'B4', 'B4', 'B4', 'A4', 'B4', 'B4', 'D4', 'C4', 'D4', 'D4', 'D4', 'D4', 'C4', 'C4', 'E4', 'E4', 'E4', 'D4']. Archive past phrases and save them to github, so even if you decide to delete it, it can be recovered in the commit history. Extract only the first phrase of a song and stick to that until your software gets it right then delete and repeat then the phrases that a method gets correct in a row is the software's accuracy score. The issue really only lies with duration. We also want the method to be rather simple and not convoluted, because of it is we can waste a lot of time again for mediocre results. What isn't guaranteed is that, phrases arent same length so won't work. We can do phrase extraction using machine learning. Then we can generate the melody and the durations using ml and ask the user if it's correct. Because it's segments, the pitch detection is correct, only that without the full context of the entire note, you get rises and falls in the vocals instead of the full note. Utilize machine learning in some way.

What about segments with harmonies/polytonality. Sometimes pitch detection mixes octaves. Segment issues. The pitches retrieved are mostly wrong. Onset detection is not correctly divided. Theoretically if could divide into pitch changes correctly. Duration tends to always be an issue. Check already written correct sheet music.
